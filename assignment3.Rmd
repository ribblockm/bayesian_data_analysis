---
title: "Assignment 3"
author: "Anonymous"
date: "25/03/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(aaltobda)
data("windshieldy1")
y <- windshieldy1
```

We should formulate the model likelihood, the prior and the posterior.
Our prior is defined as $$p(\mu, \sigma) = \sigma^{-1}$$ or $$p(\mu, \sigma^2) = \sigma^{-2}$$.
We model the likelihood as Normal with unknown $$\sigma$$. That is, $$p(y | \mu, \sigma^2) = \sigma^{-n} exp(\frac{-1}{2\sigma^2} [(n-1)s^2 + n(\bar y - \mu)^2])$$.
And the resulting posterior becomes $$p(\mu, \sigma^2 | y) = \sigma^{-n-2} exp(\frac{-1}{2\sigma^2} [(n-1)s^2 + n(\bar y - \mu)^2])$$.

Substituting some parameters in our model, the sample mean, the sample variance, and $$n$$. Our posterior is, then: $$p(\mu, \sigma^2 | y) = (\sigma^-11)*exp(-(1/2*\sigma^2)*(8*2.17 + 8*(14.61-\mu)^2))$$.

```{r}
smu <- mean(windshieldy1)
svar <- var(windshieldy1)
n <- length(windshieldy1)
```


# Exercise 1)

## a)

We can calculate the posterior mean of $$\mu$$, by integrating out the variance. The resultant integral for $$p(\mu | y)$$ is an unnormalized gamma integral, the $$t_{n-1}(\bar y, s^2/n)$$ density. Our point estimate, the posterior mean, in this case is just the sample mean of the data, $$\bar y$$.

```{r}
mu_point_est <- function(data) {
    smu <- mean(data)
    smu
}
mu_point_est(data = y)
```

We, now, plot the density.

```{r}
n <- length(y)
draws_std <- rt(10^5, n-1)
draws_nonstd <- draws_std*sqrt(var(y)) + mean(y)
t_scaled <- density(draws_nonstd, n = 10^5)
plot(t_scaled)
```

Now, we calculate the posterior interval.

```{r}
mu_interval <- function(data, prob) {
    upp <- prob + (1-prob)/2
    low <- (1-prob)/2
    qts_p <- qt(upp, n+1)
    qts_n <- qt(low, n+1)
    qtp <- smu + qts_p*sqrt(svar/(n+1))
    qtn <- smu + qts_n*sqrt(svar/(n+1))
    c(qtn, qtp)
}
mu_interval(y, .95)
```

## b)

We have that the posterior predictive distribution, $$p(\tilde y | y) = t_{n-1}(\tilde y | \bar y, (1 + \frac1n)*s^2)$$.

Again we have that the point estimate, the predictive posterior mean, is just the sample mean.

```{r}
mu_pred_point_est <- function(data) {
    smu <- mean(data)
    smu
}
mu_pred_point_est(data = y)
```

For the predictive interval, we have:

```{r}
mu_pred_interval <- function(data, prob) {
    upp <- prob + (1-prob)/2
    low <- (1-prob)/2
    qts_p <- qt(upp, n+1)
    qts_n <- qt(low, n+1)
    qtp <- smu + qts_p*sqrt(svar*(1+(1/(n+1))))
    qtn <- smu + qts_n*sqrt(svar*(1+(1/(n+1))))
    c(qtn, qtp)
}
mu_pred_interval(y, .95)
```

# Exercise 2)

Control group: $$y_0 = 39 \text{ and } n_0 = 674$$.
Treatment group: $$y_1 = 22 \text{ and } n_1 = 680$$.

The likelihood of the model follows a binomial for each group, $$p(y | p_0) = Bin(p_0, 674) = p_0^{39}*(1-p_0)^{674-39}$$ for the control group, and $$p(y | p_1) = Bin(p_1, n_1) = p_1^{22}*(1-p_1)^{680-22}$$ for the treatment group.
We use the same noninformative prior for both, $$p(p_0, p_1) \propto 1$$.
For the posterior, we obtain for the control group $$Beta(39+1, 674-39+1)$$, and $$Beta(22+1, 680-22+1)$$ for the treatment group.

## a)

We should sample from the posterior Beta distributions, and compute the odds ratio posterior distribution.

```{r}
draws_p0 <- rbeta(10^5, 40, 636)
draws_p1 <- rbeta(10^5, 23, 659)
odds_ratio <- (draws_p1/(1-draws_p1))/(draws_p0/(1-draws_p0))
```

```{r}
posterior_odds_ratio_point_est <- function(p0, p1) {
    mean(odds_ratio)
}
posterior_odds_ratio_point_est(draws_p0, draws_p1)
```

The comparison between groups through the odds ratio indicates that the probability of dying compared to the probability of not dying is almost 57% lower in the treatment group. Assuming uninformative priors for $$p_0 \text{ and } p_1$$, the odds ratio for the treatment group is probably (0.569) lower than the odds ratio for the control group.

```{r}
posterior_odds_ratio_interval <- function(p0, p1, prob) {
    odds_ratio_sorted <- sort(odds_ratio)
    upp <- prob + (1-prob)/2
    low <- (1-prob)/2
    c(odds_ratio_sorted[low*100000], odds_ratio_sorted[upp*100000])
}
posterior_odds_ratio_interval(draws_p0, draws_p1, .95)
```

The .95 credible (posterior) interval [0.32, 0.92] gives 95% of probability that the true effect lies in the interval, so the odds ratio for the treatment group is probably (.95) lower than for the control group.

```{r}
hist(odds_ratio, col = "blue")
```

## b)

# Exercise 3)

```{r}
data("windshieldy1")
data("windshieldy2")
```

We should formulate the model likelihood, the prior and the posterior.
Our prior is defined as $$p(\mu, \sigma) = \sigma^{-1}$$ or $$p(\mu, \sigma^2) = \sigma^{-2}$$.
We model the likelihood as Normal with unknown $$\sigma$$. That is, $$p(y | \mu, \sigma^2) = \sigma^{-n} exp(\frac{-1}{2\sigma^2} [(n-1)s^2 + n(\bar y - \mu)^2])$$.
And the resulting posterior becomes $$p(\mu, \sigma^2 | y) = \sigma^{-n-2} exp(\frac{-1}{2\sigma^2} [(n-1)s^2 + n(\bar y - \mu)^2])$$.

Substituting some parameters in our model, the sample mean, the sample variance, and $$n$$. Our posterior for the production line 1 is: $$p(\mu, \sigma^2 | y) = \sigma^{-11}*exp(-(1/2*\sigma^2)*(8*2.1731 + 9*(14.61-\mu)^2))$$.

And accordingly for the production line 2: $$p(\mu, \sigma^2 | y) = \sigma^{-15}*exp(-(1/2*\sigma^2)*(12*0.76 + 13*(15.82-\mu)^2))$$.

## a)

```{r}
smu1 <- mean(windshieldy1)
smu2 <- mean(windshieldy2)
svar1 <- var(windshieldy1)
svar2 <- var(windshieldy2)
n1 <- length(windshieldy1)
n2 <- length(windshieldy2)
```


First we make draws from a t(0, 1) distribution, then scale it accordingly, and then calculate the differences between the group means.

```{r}
draws_y1 <- rt(10^5, n1-1)
draws_y2 <- rt(10^5, n2-1)
y1_scaled <- draws_y1*sqrt(svar1/n1) + smu1
y2_scaled <- draws_y2*sqrt(svar2/n2) + smu2
y_diff <- y1_scaled - y2_scaled
```

The point estimate I choose is the posterior mean, which is `r mean(y_diff)`, and the posterior interval is [-2.42, 0.0014]:

```{r}
y_diff_sorted <- sort(y_diff)
c(y_diff_sorted[2750], y_diff_sorted[97250])
```

```{r}
hist(y_diff_sorted, col = "blue")
```

Summarizing the results, we have that the production line 1 have probably (-1.20) windshields that are less hard. And we have 95% of probability that the true difference in means lies in the interval [-2.42, 0.0014]. Assuming the priors as we did, uninformative.

## b)

The probability that the means are the same is zero, given that we have two continuous distribution that makes a continuous distribution for the difference.